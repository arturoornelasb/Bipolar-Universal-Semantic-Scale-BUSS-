# Copyright 2025 Jos√© Arturo Ornelas Brand

#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at

#       http://www.apache.org/licenses/LICENSE-2.0

#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.



(buss_env) ornelord@ornelord-A520M-K-V2:~/buss_project$ python buss_lora_training_v5.py
--- 1. Cargando datos reales desde aclImdb ---
Se cargaron 2000 cr√≠ticas positivas (Standard).
Se cargaron 2000 cr√≠ticas negativas (Bipolar_Opposite).
Loading model: microsoft/DialoGPT-small
LoRA model configured.
trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867
BipolarDataset inicializado: 2000 Standard, 2000 Opposite.
üß† Training BUSS-LoRA v5 (IMDB) (Epochs: 3, Bipolar Weight: 0.001, LR: 5e-05)...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.
  0%|                                                                                                                                                                                                                | 0/750 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
{'loss': 42.5721, 'grad_norm': 12.096627235412598, 'learning_rate': 2.45e-05, 'epoch': 0.2}                                                                                                                                                  
{'loss': 36.234, 'grad_norm': 9.312945365905762, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.4}                                                                                                                                      
{'loss': 25.3331, 'grad_norm': 7.127789497375488, 'learning_rate': 4.6230769230769234e-05, 'epoch': 0.6}                                                                                                                                     
{'loss': 20.909, 'grad_norm': 3.6257529258728027, 'learning_rate': 4.238461538461539e-05, 'epoch': 0.8}                                                                                                                                      
{'loss': 19.6591, 'grad_norm': 4.93461799621582, 'learning_rate': 3.853846153846154e-05, 'epoch': 1.0}                                                                                                                                       
{'loss': 19.1665, 'grad_norm': 3.7686266899108887, 'learning_rate': 3.4692307692307694e-05, 'epoch': 1.2}                                                                                                                                    
{'loss': 18.8812, 'grad_norm': 3.911139965057373, 'learning_rate': 3.084615384615385e-05, 'epoch': 1.4}                                                                                                                                      
{'loss': 18.7563, 'grad_norm': 3.2541909217834473, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.6}                                                                                                                                    
{'loss': 18.5663, 'grad_norm': 3.6612844467163086, 'learning_rate': 2.3153846153846155e-05, 'epoch': 1.8}                                                                                                                                    
{'loss': 18.3147, 'grad_norm': 3.495190143585205, 'learning_rate': 1.930769230769231e-05, 'epoch': 2.0}                                                                                                                                      
{'loss': 18.3735, 'grad_norm': 3.5663223266601562, 'learning_rate': 1.5461538461538463e-05, 'epoch': 2.2}                                                                                                                                    
{'loss': 18.2815, 'grad_norm': 3.5436794757843018, 'learning_rate': 1.1615384615384615e-05, 'epoch': 2.4}                                                                                                                                    
{'loss': 18.0967, 'grad_norm': 3.442692995071411, 'learning_rate': 7.76923076923077e-06, 'epoch': 2.6}                                                                                                                                       
{'loss': 18.1815, 'grad_norm': 3.386687755584717, 'learning_rate': 3.923076923076923e-06, 'epoch': 2.8}                                                                                                                                      
{'loss': 18.1212, 'grad_norm': 3.2864990234375, 'learning_rate': 7.692307692307692e-08, 'epoch': 3.0}                                                                                                                                        
{'train_runtime': 373.7693, 'train_samples_per_second': 32.105, 'train_steps_per_second': 2.007, 'train_loss': 21.963113444010418, 'epoch': 3.0}                                                                                             
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [06:13<00:00,  2.01it/s]
‚úÖ Training complete. Saving model to: ./buss_lora_final_v5_imdb
Model saved successfully.

--- TEST: GENERATION (v5) ---
Generated (Anchor Prompt - Positive):
   This movie was absolutely fantastic and I can't wait for it to come out to the theaters near mecca. I'm so excited to see it in theatres. It's such a great movie.
Generated (Opposite Prompt - Negative):
   Bipolar_Opposite: This movie was absolutely terrible. It was like a parody of a movie. I don't know what I was expecting, but it was nothing like the movie I watched.
(buss_env) ornelord@ornelord-A520M-K-V2:~/buss_project$ 
